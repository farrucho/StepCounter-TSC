{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9acba223",
   "metadata": {},
   "source": [
    "#### 1 - Inspect the scripts provided in the course page. Identify the layers of the neural network, as well as the relevant instructions for evaluating the MX-compatible formats. It is also recommended to read the specifications regarding the MX formats and data types, presented in this document (OCP Microscaling Formats (MX) Specification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372ad74",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "```\n",
    "self.conv1 = mx.Conv2d(1, 32, 3, 1, mx_specs=mx_specs)\n",
    "self.conv2 = mx.Conv2d(32, 64, 3, 1, mx_specs=mx_specs)\n",
    "self.dropout1 = nn.Dropout(0.25)\t\n",
    "self.dropout2 = nn.Dropout(0.5)\t\n",
    "self.fc1 = mx.Linear(9216, 128, mx_specs=mx_specs)\n",
    "self.fc2 = mx.Linear(128, 10, mx_specs=mx_specs)\n",
    "\n",
    "(...)\n",
    "\n",
    "x = self.conv1(x)\n",
    "x = mx.relu(x, mx_specs=self.mx_specs)\n",
    "x = self.conv2(x)\n",
    "x = mx.relu(x, mx_specs=self.mx_specs)\n",
    "x = F.max_pool2d(x, 2)              \n",
    "x = self.dropout1(x)\n",
    "x = torch.flatten(x, 1)             \n",
    "x = self.fc1(x)\n",
    "x = mx.relu(x, mx_specs=self.mx_specs)\n",
    "x = self.dropout2(x)\n",
    "x = self.fc2(x)\n",
    "output = mx.simd_log(mx.softmax(x, dim=1, mx_specs=self.mx_specs), mx_specs=self.mx_specs)  \n",
    "```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f1ae0",
   "metadata": {},
   "source": [
    "| Layer Order | Layer Type | Implementation | Configuration | MX-Compatible? |\n",
    "| :--- | :--- | :--- | :--- | :---: |\n",
    "| 1 | Convolution | `mx.Conv2d` | Input: 1, Output: 32, Kernel: 3x3 | Yes |\n",
    "| 2 | Activation | `mx.relu` | Elementwise ReLU | Yes |\n",
    "| 3 | Convolution | `mx.Conv2d` | Input: 32, Output: 64, Kernel: 3x3 | Yes |\n",
    "| 4 | Activation | `mx.relu` | Elementwise ReLU | Yes |\n",
    "| 5 | Pooling | `F.max_pool2d` | Max Pooling (2x2) | No |\n",
    "| 6 | Dropout | `nn.Dropout` | Probability: 0.25 | No |\n",
    "| 7 | Flatten | `torch.flatten` | Reshapes feature maps to vector | No |\n",
    "| 8 | Fully Connected | `mx.Linear` | Input: 9216, Output: 128 | Yes |\n",
    "| 9 | Activation | `mx.relu` | Elementwise ReLU | Yes |\n",
    "| 10 | Dropout | `nn.Dropout` | Probability: 0.5 | No |\n",
    "| 11 | Fully Connected | `mx.Linear` | Input: 128, Output: 10 | Yes |\n",
    "| 12 | Output | `mx.softmax` + `mx.simd_log` | LogSoftmax | Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e53e8",
   "metadata": {},
   "source": [
    "In order to emulate different accelerators we need to change the mx_specs dictionary in the `main()` function of `pytorch_code_mx.py`, that will be called as the `mx_specs=` argument on the mx functions . \n",
    "The main configurations are:\n",
    "*   **Format of matrix multiplication:**\n",
    "    *   `mx_specs['w_elem_format']`: Weight precision\n",
    "    *   `mx_specs['a_elem_format']`: Input activation precision.\n",
    "*   **Elementwise Formats:** The precision for operations like ReLU and Softmax is controlled by:\n",
    "    *   `mx_specs['bfloat']`: If non-zero, uses Brain Float format.\n",
    "    *   `mx_specs['fp']`: If `bfloat` is `0`, this defines the total bit-width for a custom floating-point format (exponent is fixed at 5 bits). Otherwise it is ignored.\n",
    "* **MX format:**\n",
    "    *   `mx_specs['block_size']`: Set to 32 to use MX formats, every 32 elemtsn share one scale factor.\n",
    "* **Shared scale:**\n",
    "    *   `mx_specs['scale_bits']`: Set to 8 Bits for the shared scale factor.\n",
    "* **CUDA acceleration:**\n",
    "    *   `mx_specs['custom_cuda']`: True if we want CUDA acceleration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad389be3",
   "metadata": {},
   "source": [
    "#### 2- Assess the impact of using some of the MX formats for matrix multiplication operations, considering different  elementwise  operations  (bfloat  and  fpX).  Note  that,  when  performing  elementwise operations with the fp option, the exponent always features a width of 5 bits, as explained in the documentation.  As  a  result,  setting  the  fp  entry  of  mx_specs  to  7  (i.e. the minimum supported value), results in a mantissa of 1 bit. Acquire the accuracy after 5 and 10 epochs for the evaluated settings.  These  results  should  be  displayed  in  the  tables  presented  in  the  last  page  of  this assignment.  Comment on your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96464557",
   "metadata": {},
   "source": [
    "| Format | int2 | int8 | fp8_e4m3 | fp8_e5m2 | bfloat16 |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **Accuracy after 5 epochs (%)** | 85 | 92  | 91 | 91  | 92  |\n",
    "| **Accuracy after 10 epochs (%)** | 82 | 93 | 92  | 92 | 92  |\n",
    "\n",
    "\n",
    "\n",
    "<small>\n",
    "\n",
    "```\n",
    "mx_specs['bfloat'] = 16\n",
    "# mx_specs['fp'] is ignored\n",
    "mx_precision = 'CHANGE THIS' # Options: 'int2', 'int8', 'fp8_e4m3', 'fp8_e5m2', 'bfloat16'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234be80",
   "metadata": {},
   "source": [
    "| Format | fp16 | fp12 | fp10 | fp8 |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Accuracy after 5 epochs (%)** | 91 | 91 | 91 | 91 |\n",
    "| **Accuracy after 10 epochs (%)** | 92 | 92 | 92 | 91 |\n",
    "\n",
    "<small>\n",
    "\n",
    "```\n",
    "mx_specs['bfloat'] = 0 # disable so we can use fp format\n",
    "mx_specs['fp'] = CHANGE THIS # Options: 16, 12, 10, 8\n",
    "mx_precision = 'fp8_e4m3'\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ae140",
   "metadata": {},
   "source": [
    "#### 3 - Consider the possibility of implementing a hardware accelerator for the presented neural network. Assuming  that  it  should  feature  an  accuracy  of  90%,  select  the  data  formats  that  allow  you to minimize  the  size  of  the  accelerator.  What accuracy did you obtain? Note: It might be useful to check which data formats are supported by the microxcaling library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba8941",
   "metadata": {},
   "source": [
    "(bruteforce.py)\n",
    "| MatMul Format   | Elem Format  | Acc @ 5    | Acc @ 10\n",
    "| :--- | :--- | :--- | :--- |\n",
    "int2            | bfloat16     | 84.93      | 86.20\n",
    "int2            | fp16         | 23.11      | 10.00\n",
    "int2            | fp12         | 81.85      | 82.23\n",
    "int2            | fp10         | 79.38      | 79.90\n",
    "int2            | fp8          | 78.82      | 80.06\n",
    "int8            | bfloat16     | 91.44      | 92.29\n",
    "int8            | fp16         | 89.41      | 88.78\n",
    "int8            | fp12         | 91.56      | 92.33\n",
    "int8            | fp10         | 91.61      | 92.40\n",
    "int8            | fp8          | 90.49      | 91.06\n",
    "fp8_e4m3        | bfloat16     | 91.27      | 92.13\n",
    "fp8_e4m3        | fp16         | 90.84      | 90.44\n",
    "fp8_e4m3        | fp12         | 91.16      | 91.87\n",
    "fp8_e4m3        | fp10         | 90.93      | 91.83\n",
    "fp8_e4m3        | fp8          | 90.61      | 91.12\n",
    "fp8_e5m2        | bfloat16     | 91.69      | 92.03\n",
    "fp8_e5m2        | fp16         | 90.48      | 83.44\n",
    "fp8_e5m2        | fp12         | 90.93      | 91.53\n",
    "fp8_e5m2        | fp10         | 90.96      | 91.95\n",
    "fp8_e5m2        | fp8          | 90.76      | 91.29\n",
    "bfloat16        | bfloat16     | 91.57      | 92.21\n",
    "bfloat16        | fp16         | 89.86      | 81.04\n",
    "bfloat16        | fp12         | 91.79      | 92.27\n",
    "bfloat16        | fp10         | 91.26      | 92.14\n",
    "bfloat16        | fp8          | 90.40      | 90.86"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d44d47",
   "metadata": {},
   "source": [
    "To achieve the target accuracy of 90% while minimizing the bit-width (size) of the accelerator, we selected the following configuration. For weights and activations we choose fp8_e4m3 (8-bit). We also evaluated int2, which performed worse (max accuracy was 86.20% with bfloat16, and much lower with fp formats). All 8-bit formats (int8, fp8_e4m3, fp8_e5m2) successfully exceeded 90%. We select fp8_e4m3 as it is the standard format for inference in the Microscaling specification (but other >90% accuracy combinations can be used). For elementwise operations we adopted fp8 (8-bit). We tested elementwise formats descending from bfloat16 down to fp8 and the results shown that the custom fp8 format (1 sign, 5 exponent, 2 mantissa) is sufficient. Even with this low precision, the network maintained high accuracy.\n",
    "With this configuration (fp8_e4m3 + fp8), we obtained an accuracy of 91.12%. \n",
    "\n",
    "By selecting fp8 for both matrix multiplications and elementwise operations, the accelerator can run entirely on 8-bit data paths. This halves the memory requirement compared to a 16-bit system (like bfloat16) and significantly reduces the complexity of Arithmetic Logic Units (ALUs), satisfying the goal of minimizing the accelerator size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f4443",
   "metadata": {},
   "source": [
    "porque Ã© que quando corremos com cpu temos NAN e quando corremos com grafica nao temos?? explicaar isso!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b737a6",
   "metadata": {},
   "source": [
    "4- (Optional) Select the data formats in order to obtain the worst possible accuracy for the considered \n",
    "network.  How  much  did  you  obtain?  Explain  your  methodology  and  comment  on  any  notable \n",
    "behaviour that you find during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00f1ad",
   "metadata": {},
   "source": [
    "Question 4 Response (Optional)\n",
    "\n",
    "1. Selected Data Formats:\n",
    "To obtain the worst possible accuracy, we identified the following combination from our grid search results:\n",
    "\n",
    "Matrix Multiplication Format: int2\n",
    "\n",
    "This limits the weights to only 2 bits (4 unique values), providing the lowest information capacity.\n",
    "\n",
    "Elementwise Format: fp16 (Custom)\n",
    "\n",
    "This uses the library's custom floating-point format with a fixed 5-bit exponent.\n",
    "\n",
    "2. Accuracy Obtained:\n",
    "We obtained an accuracy of 10.00% after 10 epochs.\n",
    "\n",
    "3. Methodology:\n",
    "We performed a comprehensive grid search across all supported matrix multiplication and elementwise format combinations. We looked for the configuration that resulted in the lowest validation accuracy. While int2 generally performed poorly compared to 8-bit formats, this specific combination resulted in a complete model failure.\n",
    "\n",
    "4. Comments on Notable Behavior:\n",
    "\n",
    "Random Guessing: Since the Fashion-MNIST dataset has 10 classes, an accuracy of 10.00% represents pure random guessing. The model completely failed to learn any distinguishing features of the images.\n",
    "\n",
    "\"Unlearning\" / Divergence: It is notable that at Epoch 5, this configuration had an accuracy of 23.11%, but by Epoch 10, it collapsed to 10.00%. This behavior is characteristic of numerical divergence. The model likely encountered exploding gradients or accumulated numerical errors (NaNs) in the later epochs, destroying whatever small patterns it had initially learned.\n",
    "\n",
    "Instability of int2: The int2 format proved to be highly unstable. While it managed to achieve ~80% accuracy when paired with robust elementwise formats (like bfloat16), it collapsed when paired with the custom fp16 format. This highlights that extremely low-precision weights (int2) require high-precision accumulators and activations to remain stable; without them, training fails."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

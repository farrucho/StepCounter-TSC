{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9acba223",
   "metadata": {},
   "source": [
    "#### 1 - Inspect the scripts provided in the course page. Identify the layers of the neural network, as well as the relevant instructions for evaluating the MX-compatible formats. It is also recommended to read the specifications regarding the MX formats and data types, presented in this document (OCP Microscaling Formats (MX) Specification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372ad74",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "```\n",
    "self.conv1 = mx.Conv2d(1, 32, 3, 1, mx_specs=mx_specs)\n",
    "self.conv2 = mx.Conv2d(32, 64, 3, 1, mx_specs=mx_specs)\n",
    "self.dropout1 = nn.Dropout(0.25)\t\n",
    "self.dropout2 = nn.Dropout(0.5)\t\n",
    "self.fc1 = mx.Linear(9216, 128, mx_specs=mx_specs)\n",
    "self.fc2 = mx.Linear(128, 10, mx_specs=mx_specs)\n",
    "\n",
    "(...)\n",
    "\n",
    "x = self.conv1(x)\n",
    "x = mx.relu(x, mx_specs=self.mx_specs)\n",
    "x = self.conv2(x)\n",
    "x = mx.relu(x, mx_specs=self.mx_specs)\n",
    "x = F.max_pool2d(x, 2)              \n",
    "x = self.dropout1(x)\n",
    "x = torch.flatten(x, 1)             \n",
    "x = self.fc1(x)\n",
    "x = mx.relu(x, mx_specs=self.mx_specs)\n",
    "x = self.dropout2(x)\n",
    "x = self.fc2(x)\n",
    "output = mx.simd_log(mx.softmax(x, dim=1, mx_specs=self.mx_specs), mx_specs=self.mx_specs)  \n",
    "```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f1ae0",
   "metadata": {},
   "source": [
    "| Layer Order | Layer Type | Implementation | Configuration | MX-Compatible? |\n",
    "| :--- | :--- | :--- | :--- | :---: |\n",
    "| 1 | Convolution | `mx.Conv2d` | Input: 1, Output: 32, Kernel: 3x3 | Yes |\n",
    "| 2 | Activation | `mx.relu` | Elementwise ReLU | Yes |\n",
    "| 3 | Convolution | `mx.Conv2d` | Input: 32, Output: 64, Kernel: 3x3 | Yes |\n",
    "| 4 | Activation | `mx.relu` | Elementwise ReLU | Yes |\n",
    "| 5 | Pooling | `F.max_pool2d` | Max Pooling (2x2) | No |\n",
    "| 6 | Dropout | `nn.Dropout` | Probability: 0.25 | No |\n",
    "| 7 | Flatten | `torch.flatten` | Reshapes feature maps to vector | No |\n",
    "| 8 | Fully Connected | `mx.Linear` | Input: 9216, Output: 128 | Yes |\n",
    "| 9 | Activation | `mx.relu` | Elementwise ReLU | Yes |\n",
    "| 10 | Dropout | `nn.Dropout` | Probability: 0.5 | No |\n",
    "| 11 | Fully Connected | `mx.Linear` | Input: 128, Output: 10 | Yes |\n",
    "| 12 | Output | `mx.softmax` + `mx.simd_log` | LogSoftmax | Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e53e8",
   "metadata": {},
   "source": [
    "In order to emulate different accelerators we need to change the mx_specs dictionary in the `main()` function of `pytorch_code_mx.py`, that will be called as the `mx_specs=` argument on the mx functions . \n",
    "The main configurations are:\n",
    "*   **Format of matrix multiplication:**\n",
    "    *   `mx_specs['w_elem_format']`: Weight precision\n",
    "    *   `mx_specs['a_elem_format']`: Input activation precision.\n",
    "*   **Elementwise Formats:** The precision for operations like ReLU and Softmax is controlled by:\n",
    "    *   `mx_specs['bfloat']`: If non-zero, uses Brain Float format.\n",
    "    *   `mx_specs['fp']`: If `bfloat` is `0`, this defines the total bit-width for a custom floating-point format (exponent is fixed at 5 bits). Otherwise it is ignored.\n",
    "* **MX format:**\n",
    "    *   `mx_specs['block_size']`: Set to 32 to use MX formats, every 32 elemtsn share one scale factor.\n",
    "* **Shared scale:**\n",
    "    *   `mx_specs['scale_bits']`: Set to 8 Bits for the shared scale factor.\n",
    "* **CUDA acceleration:**\n",
    "    *   `mx_specs['custom_cuda']`: True if we want CUDA acceleration.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
